%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AMS Beamer series / Bologna FC / Template
% Andrea Omicini
% Alma Mater Studiorum - Universit√† di Bologna
% mailto:andrea.omicini@unibo.it
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass[handout]{beamer}\mode<handout>{\usetheme{default}}
%
\documentclass[presentation, 9pt]{beamer}\mode<presentation>{\usetheme{AMSBolognaFC}}
%\documentclass[handout]{beamer}\mode<handout>{\usetheme{AMSBolognaFC}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[T1]{fontenc}
\usepackage{wasysym}
\usepackage{amsmath,blkarray}
\usepackage[minted,most]{tcolorbox}
\usepackage{centernot}
\usepackage{fontawesome}
\usepackage{fancyvrb}
\usepackage{minted}
\setminted[scala]{baselinestretch=1,obeytabs=true, tabsize=2}
\usepackage[ddmmyyyy]{datetime}
\definecolor{darkcyan}{rgb}{0.0, 0.55, 0.55}
\renewcommand{\dateseparator}{}
%\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\version}{1}
\usepackage[
	backend=biber,
	citestyle=authoryear-icomp,
	maxcitenames=1,
	bibstyle=numeric]{biblatex}

	\makeatletter

\addbibresource{biblio.bib}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Next steps in Aggregate Computing and Machine Learning integration
\title[Next steps -- AC + ML]
{Next steps in Aggregate Computing and Machine Learning integration}
%
\subtitle[Recap and perspectives]
{Recap and perspectives}
%
\author[\sspeaker{Aguzzi}]
{\speaker{Gianluca Aguzzi} \href{mailto:gianluca.aguzzi@unibo.it}{gianluca.aguzzi@unibo.it}}
%
\institute[DISI, Univ.\ Bologna]
{Dipartimento di Informatica -- Scienza e Ingegneria (DISI)\\
\textsc{Alma Mater Studiorum} -- Universit{\`a} di Bologna \\[0.5cm]
}
%
\renewcommand{\dateseparator}{/}
\date[\today]{\today}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%/////////
\frame{\titlepage}
%/////////
%===============================================================================
\section{Recap}
\begin{frame}{AC \faPlus \, ML: Current status}
\begin{itemize}
	\item In \citetitle{research} we highlight three main integrations perspective:
	\begin{itemize}
		\item Inject learning at the algorithm definition level
		\item Use learning to find execution strategies
		\item Learn how to deploy aggregate computing programs
	\end{itemize}
	\item Last year contributions:
	\begin{itemize}
		\item Program sketching via (Multi-Agent) Reinforcement Learning
		\item Distributed schedulers via Reinforcement Learning
	\end{itemize}
	\item This year's perspective:
	\begin{itemize}
		\item Finalize the two conference papers into a journal ``state''
		\begin{itemize}
			\item Today focus -- discussion on possible contributions/interventions to conclude these
		\end{itemize}
		\item Reach a conference paper for the pulverised deployment
	\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}{Reinforcement Learning for Execution Strategy}
	\begin{alertblock}{Addressing Collective Computations Efficiency}
		\begin{itemize}
			\item \bold{Main contribution}: tune the round frequency in order to reduce the power consumption of a self-stabilising program
			\item \bold{How}: Shared Q table update with local experience (\emph{Homogenous behaviour})
			\begin{itemize}
				\item Each agent perceives local output trajectory
				\item The local state depends on the historical variation of that output
				\item The actions that the agents could perform consist of the next wake-up time
				\item The reward depends on the variation of the output
				\begin{itemize}
					\item if the output is stable, the lower the next wake-up time is the higher the reward
					\item if the output is unstable, the higher the next wake-up time is the lower the reward
				\end{itemize}
			\end{itemize}
		\end{itemize}
	\end{alertblock}
	\begin{block}{Critics}
		\begin{itemize}
			\item Multi-objective nature not explored
			\item Simulations/experiments are not enough (too few agents)
			\item Q-learning too naive for this kind of problem
		\end{itemize}
	\end{block}
\end{frame}
%===============================================================================
\begin{frame}{Reinforcement Learning for Execution Strategy -- ideas}
\begin{itemize}
		\item Improve the policy:
		\begin{itemize}
			\item Currently, the actions are limited to the next wake-up time.
			\item The actions instead, could be any kind of event (also a set of them)
			\begin{itemize}
				\item A certain delta change in a sensor
				\item A trigger generated from a timer each dt 
			\end{itemize}
			\item In doing this, we might try to use a more advanced RL algorithm (like the suggested A3C)
		\end{itemize}
		\item Compare our solution with Distributed Schedulers
		\begin{itemize}
			\item This will create a robust evaluation
		\end{itemize}
		\item Generalize the reward function to not only self-stabilising programs
		\item We could use the neighbourhood information to encode the agent state
		\item Possible issues
		
\end{itemize}
\begin{block}{Possible issues}
\begin{itemize}
	\item Not clear how to combine events as the policy output
	\item Deep Learning will slow down the learning phases
	\item The state information could be too simple in order to get benefits from DL
	\item Not easy to find the right reward function 
\end{itemize}
\end{block}
\end{frame}
\begin{frame}{Reinforcement Learning for Distributed Algorithms}
\begin{alertblock}{Towards reinforcement learning-based aggregate computing}
	\begin{itemize}
		\item \textbf{Main Contribution}: sketch aggregate computing algorithm leaving holes for the unknown/complex parts, that will be filled with experience through multi-agent reinforcement learning
		\item \textbf{How}: Shared-Q table updates using a modified version of Hysteretic Q-Learning
		\begin{itemize}
			\item Similarly to the previous approach, each agent follows a local policy that is updated with local experience
			\item That update influences a global Q table
			\item At runtime, the same Q-table is shared with the whole system
		\end{itemize}
		\item \bold{Problems encountered}: not easy to understand \emph{why} the policy work, the state discretization is not enough to capture the state of the system, 
	\end{itemize}
\end{alertblock}
\begin{block}{Critics}
\begin{itemize}
	\item Centralised training -- not scalable
	\item Not clear how it works in different scenarios/environments
	\item Evaluation was too simple
\end{itemize}
\end{block}
\end{frame}
\begin{frame}{Reinforcement Learning for Distributed Algorithms -- ideas}
\begin{itemize}
	\item Improve the policy \faArrowRight \, two ways:
	\begin{itemize}
		\item Extract explainable policy -- this will help to understand how the agent will react against the model update
		\item Use deep learning approach -- this will produce complex policy that will scale easier with the complexity of the problem
	\end{itemize}
	\item Case study:
	\begin{itemize}
		\item Currently we only try the founded policy in the same environment
		\item We could use it in a high-level use case to show the benefit of it
		\item We could consider other building
		\item We could consider other reward functions that optimise other aspects
		\item Limitations:
		\begin{itemize}
			\item DL methods work better with	continuous state space and with a lot of information \faArrowRight \, In our case, the agent state is built upon the previous output of the algorithm
			\item The reward is delayed \faArrowRight \, the agent state cannot handle the long-term trajectory that will lead it to a good state.
		\end{itemize}
	\end{itemize}
	\item Nice to have \emph{online} \& \emph{distributed} training
	\begin{itemize}
		\item A better match with the AC philosophy
		\item It should scale better with the number of agents
		\item[\faThumbsDown] unclear how to define the reward function
		\begin{itemize}
			\item Federated learning could be a solution?
		\end{itemize}
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Reinforcement Learning for System structure \& restructuring}
	\begin{itemize}
		\item Aggregate program partitioning is mainly done at design time
		\item Use ML to learn the best deployment in order to improve a certain non-funcitonal/functional requirement defined by an objective function
		\item At design time: evolutionary algorithm / genetic algorithms
		\begin{itemize}
			\item Given a certain topology, map each pulverised block of a program to a certain computational node
		\end{itemize}
		\item At runtime: RL that moves opportunistically the computation in other nodes (from cloud to edge and vice versa)
		\begin{itemize}
			\item Each logical computation knows \emph{where} it could be placed, and opportunistically move itself to a better `place'
			\item Same problems of the previous approaches: not easy to translate the objective function into a reward function, possibly a non-stationary environment, agents need to coordinate in order to find the best policy.
		\end{itemize}
	\item publication perspective:
	\begin{itemize}
		\item We start from the pulverised simulation presented in the paper (or we could use Farabegoli's work?)
		\item We integrate one of the techniques that will decide
		\item Probably the focus should be on the opportunistic migration of the computation 
	\end{itemize}
\end{itemize}
\end{frame}
%===============================================================================
\section*{}
%=================

%===============================================================================
\section*{\refname}
%===============================================================================

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
